{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fa835e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from matplotlib import animation\n",
    "import cv2\n",
    "import torch\n",
    "from IPython.display import display,HTML\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "import torch.multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "#https://shmuma.medium.com/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55\n",
    "# import gymnasium as gym\n",
    "\n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/36.0, frames[0].shape[0]/36.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole.mp4')\n",
    "    return anim\n",
    "#     display(display_animation(anim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5bdca45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "# device = torch.device(\"cpu\")\n",
    "    \n",
    "# 将图像转换为64*64\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        old_space = self.observation_space\n",
    "        self.obervation_space = gym.spaces.Box(\n",
    "            self.observation(old_space.low),\n",
    "            self.observation(old_space.high),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        new_obs = cv2.resize(\n",
    "            obs, (64, 64)\n",
    "        )\n",
    "        # 转置 (210, 160, 3) -> (3, 210, 160)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "87b61974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储经验\n",
    "class ReplayMemory:\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, state_next, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "#         if reward != 0:\n",
    "#             self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "#             self.index = (self.index + 1) % self.capacity\n",
    "#         elif reward == 0 and 0.6 <= np.random.uniform(0,1):\n",
    "#             self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "#             self.index = (self.index + 1) % self.capacity\n",
    "#         else:\n",
    "#             if self.memory[self.index] is None:\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "868c4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_actions=4):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "#         conv_out_size = self._get_conv_out(in_channels)\n",
    "#         o = self.conv(Variable(torch.zeros(1, *shape)))\n",
    "#         int(np.prod(o.size()))\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9eaeabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "class Brain:\n",
    "    def __init__(self, num_state, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "#         print('state {}, action: {}'.format(num_state, num_actions))\n",
    "#         1, 3, 64, 64]\n",
    "        self.dqn = ImageModel(num_actions=num_actions).to(device)\n",
    "        self.target_model = ImageModel(num_actions=num_actions).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr = 0.00025)\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "#             print('minimal batch size：', len(self.memory))\n",
    "            return 0\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state).to(device)#.cuda(non_blocking=True)\n",
    "#         print('batch', state_batch.shape)\n",
    "        action_batch =torch.cat(batch.action).to(device)#.cuda(non_blocking=True)\n",
    "#         print('reward', batch.reward)\n",
    "        reward_batch =torch.FloatTensor([[r] for r in batch.reward]).to(device)#.cuda(non_blocking=True)\n",
    "#         print('batch.reward:',batch.reward)\n",
    "        # 在状态S下执行动作action_batch后得到的新状态next_S\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)#.cuda(non_blocking=True)\n",
    "\n",
    "#         print('state: ',state_batch.shape, 'next state:', non_final_next_states.shape )\n",
    "        # 切换推理模式，计算Q值\n",
    "        self.dqn.eval()\n",
    "        # 计算Q值:使用gather函数从输出中提取对应于实际采取的动作的小批量变量action的输出\n",
    "#         dqn_action_values = self.dqn(state_batch)\n",
    "#         print('batch 2', dqn_action_values.shape)\n",
    "#         next_action_values = self.target_model(non_final_next_states)\n",
    "        # 对DQN做正向传播，得到q_j\n",
    "        state_action_values = self.get_dqn_q(state_batch, action_batch)\n",
    "#         print('action value', state_action_values.shape)\n",
    "        # 最大Q值对应的动作\n",
    "        next_state_action_values = self.get_dqn_q(non_final_next_states, action_batch)\n",
    "        max_value_actions = self.select_argmax_action(next_state_action_values)\n",
    "        # 求下一个状态下最大Q值\n",
    "#         print('next state:', non_final_next_states, max_value_actions.unsqueeze(1))\n",
    "        self.target_model.eval()\n",
    "        next_state_values = self.get_target_q(non_final_next_states, max_value_actions.unsqueeze(1))\n",
    "\n",
    "        expectd_state_action_values = (reward_batch + (self.gamma * next_state_values)).detach()  # Maximize Q\n",
    "#         print('cur {}, dst: {}'.format(state_action_values, expectd_state_action_values))\n",
    "#         print('reward_batch:', reward_batch.shape, 'exected: ', next_state_values.shape, 'squeeze:', expectd_state_action_values.unsqueeze(1).shape)\n",
    "        # 切换到训练模式\n",
    "        self.dqn.train()\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expectd_state_action_values)\n",
    "#         loss = F.smooth_l1_loss(state_action_values, expectd_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad() # Backpropagation: clear the tensors from previous gradients calculations.\n",
    "        loss.backward() # Backpropagation: calculate the gradients.\n",
    "        self.optimizer.step() # Updating the weights.\n",
    "        \n",
    "        self.soft_update()\n",
    "        return loss\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 1/(episode + 1)   # 采用e-贪婪法逐步采用最优动作\n",
    "        if epsilon < 0.1:\n",
    "            epsilon = 0.1\n",
    "\n",
    "        if epsilon <= np.random.uniform(0,1):\n",
    "            self.dqn.eval()\n",
    "            with torch.no_grad():\n",
    "                value = self.dqn(state.to(device))\n",
    "#                 print('shape:', value.shape, value2)\n",
    "                action = value.max(1)[1].view(1,1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.num_actions)]]).to(device)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_dqn_q(self, state_batch, action_batch):\n",
    "        # 计算Q值:使用gather函数从输出中提取对应于实际采取的动作的小批量变量action的输出\n",
    "        dqn_action_values = self.dqn(state_batch)\n",
    "        # state_action_values是状态S在网络模型Q下的所有动作的预测值\n",
    "        return dqn_action_values.gather(1, action_batch)\n",
    "    \n",
    "    def get_target_q(self, state_batch, action_batch):\n",
    "        # 计算Q值:使用gather函数从输出中提取对应于实际采取的动作的小批量变量action的输出\n",
    "        action_values = self.target_model(state_batch)\n",
    "#         print('shape:', action_values.shape, action_batch.shape)\n",
    "        # state_action_values是状态S在网络模型Q下的所有动作的预测值\n",
    "        return action_values.gather(1, action_batch)\n",
    "    \n",
    "    def select_argmax_action(self, q):\n",
    "        # 选择使q最大的那个动作\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, q)))\n",
    "        next_state_values = torch.zeros(BATCH_SIZE).to(device)\n",
    "        # 最大q值对应的动作\n",
    "#         print('max action', q.shape)\n",
    "        return q.max(1)[1].detach()\n",
    "#         return q.max(1)[1].detach()\n",
    "    \n",
    "    def soft_update(self, tau=0.01):\n",
    "        for target_param, param in zip(self.target_model.parameters(), self.dqn.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2fa615a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_state, num_actions):\n",
    "        self.brain = Brain(num_state, num_actions)\n",
    "        \n",
    "    def update_q_function(self):\n",
    "        return self.brain.replay()\n",
    "    \n",
    "    def get_action(self, state, episode):\n",
    "        return self.brain.decide_action(state, episode)\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "84d83907",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100000\n",
    "max_steps = 2000\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = InputWrapper(gym.make(\"BreakoutNoFrameskip-v4\", render_mode='rgb_array'))\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        # 0-不动 1- 重置 2-向右 3- 向左\n",
    "#         print('action', self.num_actions)\n",
    "#         print('num_states', self.num_states)\n",
    "#         print('observation_space', self.env.observation_space)\n",
    "        self.agent = Agent(self.num_states, self.num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        record = False\n",
    "        complete_episode = 0\n",
    "        max_record = 0\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            observation = self.env.reset()[0]\n",
    "#             print('observation', observation.shape)\n",
    "            state = observation\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            state = torch.unsqueeze(state,0)\n",
    "            self.frames = []\n",
    "            \n",
    "            total_reward = 0\n",
    "            for step in range(0, max_steps):\n",
    "                self.frames.append(self.env.render())\n",
    "                action = self.agent.get_action(state, episode)\n",
    "                observation_next, reward, terminated, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    reward = 100\n",
    "                else:\n",
    "                    reward = reward/500\n",
    "                if info['lives'] >=2:\n",
    "                    state_next = observation_next\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)\n",
    "                    state_next = torch.unsqueeze(state_next,0)\n",
    "\n",
    "                    self.agent.memorize(state.to(device), action.to(device), state_next.to(device), reward/50)\n",
    "                    loss = self.agent.update_q_function()\n",
    "                    state = state_next            # 保存当前状态到历史记录中\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if episode % 50 == 0:\n",
    "                print('{} {} Episode: finish after {} steps, total reward {}, last loss: {}'.format(datetime.now().strftime('%H:%M:%S'), episode, step + 1, total_reward, loss))\n",
    "            if total_reward > max_record:\n",
    "                display_frames_as_gif(self.frames)\n",
    "                max_record = total_reward\n",
    "\n",
    "            if complete_episode >= 10 or episode == num_episodes - 2:\n",
    "                record = True\n",
    "            \n",
    "    def display(self):\n",
    "        gif = display_frames_as_gif(self.frames)\n",
    "        display(display_animation(gif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35793340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:13:15 0 Episode: finish after 692 steps, total reward 2.0, last loss: 5.9985344705637544e-05\n",
      "00:17:50 50 Episode: finish after 392 steps, total reward 0.0, last loss: 5.482451342686545e-06\n",
      "00:21:29 100 Episode: finish after 439 steps, total reward 0.0, last loss: 7.901838898760616e-07\n",
      "00:25:24 150 Episode: finish after 399 steps, total reward 0.0, last loss: 2.6894166147567455e-12\n",
      "00:28:38 200 Episode: finish after 598 steps, total reward 1.0, last loss: 7.078169994567673e-11\n",
      "00:31:43 250 Episode: finish after 597 steps, total reward 1.0, last loss: 1.0844474623850076e-11\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('GymV26Environment-v0', env_id = \"BreakoutNoFrameskip-v4\")\n",
    "# env = gym.make(\"Breakout-v0\")\n",
    "env = Environment()\n",
    "env.run()\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 1)\n",
    "# a = torch.randn(2, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6d0ec53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5453],\n",
       "        [-0.1009]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[2],[1]]) + 1*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "61a0812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5453"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 -0.4547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c7000f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7627, 1.2305]),\n",
       "indices=tensor([3, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9054552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69fad636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(map(lambda s: s is not None, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5b744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7627, 1.2305])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webbe\\AppData\\Local\\Temp\\ipykernel_9576\\4124750931.py:3: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  nsv[non_final_mask] = torch.max(a, 1)[0].detach()\n"
     ]
    }
   ],
   "source": [
    "nsv = torch.zeros(2)\n",
    "# 求下一个状态下最大Q值\n",
    "nsv[non_final_mask] = torch.max(a, 1)[0].detach()\n",
    "print(nsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f4d9f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7627],\n",
       "        [1.2305]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsv.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0338884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
