{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cbff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566b7799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 3 Box(-2.0, 2.0, (1,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_info = env.action_space\n",
    "print(env.observation_space.shape, obs_dim, action_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c222b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(ActionNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)\n",
    "    \n",
    "class QValueNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        '''状态价值，用来评估动作的好坏程度'''\n",
    "        super(QValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        t = torch.cat([x, a], 0)\n",
    "        return self.layers(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdb4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActionNetwork(obs_dim, 1)\n",
    "critic = QValueNetwork(obs_dim, 1)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943fbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''策略网络做出决策，给出一个动作'''\n",
    "    action = actor(torch.FloatTensor(state).to(device))\n",
    "    return action\n",
    "\n",
    "def critic_value(state, action):\n",
    "    return critic(torch.FloatTensor(state).to(device), action.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664f3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0805], grad_fn=<AddBackward0>) tensor([-9.5283], grad_fn=<AddBackward0>)\n",
      "tensor([0.0723], grad_fn=<AddBackward0>) tensor([-9.3830], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03846d6f5982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m'''更新价值网络'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "steps = []\n",
    "U_s = []\n",
    "view_losses = []\n",
    "max_epoch = 2000\n",
    "for i in range(max_epoch):\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    state, _ = env.reset(seed=3)\n",
    "    trajectories = []\n",
    "    while True:\n",
    "        '''让策略网络做预测'''\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(np.float32(action.detach().numpy()))\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        trajectories.append([state, action, reward, next_state, done])\n",
    "        if done or step > 200:\n",
    "            steps.append(step)\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    # 结束一轮游戏\n",
    "    for state, action, reward, next_state, done in trajectories[::-1]:        \n",
    "        mask = 1 - done\n",
    "#         print('---------', torch.tensor(state), action, torch.tensor(state).shape, action.shape)\n",
    "#         '''让价值网络做预测'''\n",
    "#         result = torch.cat([torch.tensor(state), action], 0)\n",
    "#         print('---------', result)\n",
    "        q_t = critic_value(state, action)\n",
    "        \n",
    "        next_action = select_action(next_state)\n",
    "        next_q_t = critic_value(next_state, next_action)\n",
    "        '''计算价值网络的 TD 目标和 TD 误差'''\n",
    "        y_t = reward + gamma * next_q_t * mask\n",
    "        print(q_t, y_t)\n",
    "        critic_loss = F.smooth_l1_loss(q_t, y_t)\n",
    "        '''更新价值网络'''\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        new_action = select_action(state)\n",
    "        actor_loss = -critic_value(state, new_action)\n",
    "    \n",
    "        '''更新策略网络'''\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察曲线\n",
    "plt.figure(figsize=(20, 5))\n",
    "# plt.subplot(131)\n",
    "# plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "# plt.title('score')\n",
    "# plt.plot(scores)\n",
    "plt.subplot(132)\n",
    "plt.title('steps')\n",
    "plt.plot(steps)\n",
    "# plt.subplot(133)\n",
    "# plt.title('epsilons')\n",
    "# plt.plot(epsilons)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
