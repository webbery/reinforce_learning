{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3cbff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566b7799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 3 Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_info = env.action_space\n",
    "print(env.observation_space.shape, obs_dim, action_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c222b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(ActionNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)\n",
    "    \n",
    "class QValueNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        '''状态价值，用来评估动作的好坏程度'''\n",
    "        super(QValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim + out_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        t = torch.cat([x, a], 1)\n",
    "        return self.layers(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bdb4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActionNetwork(obs_dim, 1)\n",
    "critic = QValueNetwork(obs_dim, 1)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "943fbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''策略网络做出决策，给出一个动作，并让智能体执行'''\n",
    "    action = actor(torch.FloatTensor(state).to(device))\n",
    "    return action.item()\n",
    "\n",
    "def critic_value(state, action):\n",
    "    return critic(torch.FloatTensor(state).to(device), torch.FloatTensor(action).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "steps = []\n",
    "U_s = []\n",
    "view_losses = []\n",
    "max_epoch = 2000\n",
    "for i in range(max_epoch):\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    state, _ = env.reset(seed=3)\n",
    "    trajectories = []\n",
    "    while True:\n",
    "        '''让策略网络做预测'''\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        trajectories.append([state, action, reward, next_state, done])\n",
    "        if done or step > 200:\n",
    "            steps.append(step)\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    # 结束一轮游戏\n",
    "    for state, action, reward, next_state, next_action, log_prob, done in trajectories[::-1]:        \n",
    "        mask = 1 - done\n",
    "        \n",
    "        '''让价值网络做预测'''\n",
    "        q_t = critic_value(state, action)\n",
    "        next_q_t = critic_value(next_state, next_action)\n",
    "        '''计算价值网络的 TD 目标和 TD 误差'''\n",
    "        y_t = reward + gamma * next_q_t * mask\n",
    "        state_loss = F.smooth_l1_loss(q_t, y_t)\n",
    "        '''更新价值网络'''\n",
    "        critic_optimizer.zero_grad()\n",
    "        state_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "    loss_sum = torch.tensor([0.0])\n",
    "    for state, action, reward, next_state, next_action, log_prob, done in trajectories[::-1]:    \n",
    "        q_t = critic_value(state, action)\n",
    "        loss_sum += -q_t.item() * log_prob\n",
    "    \n",
    "    '''更新策略网络'''\n",
    "    actor_optimizer.zero_grad()\n",
    "    loss_sum.backward()\n",
    "    actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察曲线\n",
    "plt.figure(figsize=(20, 5))\n",
    "# plt.subplot(131)\n",
    "# plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "# plt.title('score')\n",
    "# plt.plot(scores)\n",
    "plt.subplot(132)\n",
    "plt.title('steps')\n",
    "plt.plot(steps)\n",
    "# plt.subplot(133)\n",
    "# plt.title('epsilons')\n",
    "# plt.plot(epsilons)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
