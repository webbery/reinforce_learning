{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4abd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Normal\n",
    "import math\n",
    "import random\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e1db45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) 3 Box(-2.0, 2.0, (1,), float32) 2.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_info = env.action_space\n",
    "max_action = float(env.action_space.high[0])\n",
    "print(env.observation_space.shape, obs_dim, action_info, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e45508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(in_dim, 400), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "        self.std = nn.Sequential(\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        val = self.base(x)\n",
    "        mean = self.mean(val)\n",
    "        std = self.std(val)\n",
    "        return mean, torch.sqrt(torch.exp(std))\n",
    "    \n",
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(VNetwork, self).__init__()\n",
    "\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(in_dim, 400), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.base(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "275520f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = QNetwork(obs_dim).to(device)\n",
    "critic = VNetwork(obs_dim).to(device)\n",
    "def select_action(state, is_test = False):\n",
    "    mean, std = actor(torch.FloatTensor(state).to(device))\n",
    "    # 得到动作策略的概率质量分布,用该分布初始化采样器\n",
    "    m = Normal(mean, std)\n",
    "    # 按该分布做采样,得到一个动作\n",
    "    action = m.sample()\n",
    "    # 对概率质量函数取对数,并采样该分布在action的值,即log P在action点的概率密度函数值\n",
    "    loss = m.log_prob(action)\n",
    "    return max_action * torch.tanh(action), loss\n",
    "\n",
    "def critic_value(state):\n",
    "    return critic(torch.FloatTensor(state).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001)\n",
    "\n",
    "steps = []\n",
    "U_s = []\n",
    "view_losses = []\n",
    "scores = []\n",
    "max_epoch = 5000\n",
    "for i in range(max_epoch):\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    state, _ = env.reset(seed=39)\n",
    "    trajectories = []\n",
    "    while True:\n",
    "        action, log_prob = select_action(state)\n",
    "        '''从环境中观测到奖励和新的状态'''\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        value = critic_value(state)\n",
    "        \n",
    "        step += 1\n",
    "        score += reward\n",
    "        \n",
    "        trajectories.append([state, action, reward, next_state, log_prob, done, value])\n",
    "        if done:\n",
    "            steps.append(step)\n",
    "            scores.append(score)\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    # 结束一轮游戏\n",
    "        \n",
    "#         '''让价值网络打分'''\n",
    "    roll_count = math.floor(len(trajectories) / 8) * 8\n",
    "    start_idx = 0\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    masks     = []\n",
    "    G = []\n",
    "    for k in range(roll_count):\n",
    "        state, action, reward, next_state, log_prob, done, value = trajectories[start_idx + k]\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(reward + gamma * value)\n",
    "        masks.append(1 - done)\n",
    "        G.append(critic_value(next_state))\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    G = torch.cat(G)\n",
    "    values = torch.cat(values)\n",
    "    advantage = G - values\n",
    "\n",
    "    actor_loss  = -(log_probs * advantage.detach()).mean()\n",
    "#         actor_loss += -(log_probs[indx] * advantage.detach()[indx])\n",
    "#     print('actor_loss', actor_loss)\n",
    "    view_losses.append(actor_loss.item())\n",
    "\n",
    "#     critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    critic_loss = F.mse_loss(G, values)\n",
    "#     print('critic_loss', critic_loss)\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察曲线\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(131)\n",
    "# plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "plt.title('score')\n",
    "plt.plot(scores)\n",
    "plt.subplot(132)\n",
    "plt.title('steps')\n",
    "plt.plot(steps)\n",
    "plt.subplot(133)\n",
    "plt.title('actor loss')\n",
    "plt.plot(view_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8596188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/webberg/workspace/code/reinforce_learning/policy folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/webberg/workspace/code/reinforce_learning/policy/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/webberg/workspace/code/reinforce_learning/policy/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/webberg/workspace/code/reinforce_learning/policy/rl-video-episode-0.mp4\n",
      "score:  -1358.848910913532 step: 201\n"
     ]
    }
   ],
   "source": [
    "# 观察效果\n",
    "def test():\n",
    "    dvideo_env = gym.wrappers.RecordVideo(env, video_folder='.')\n",
    "    state, _ = dvideo_env.reset(seed=3)\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action, _ = select_action(state)\n",
    "        next_state, reward, done, truncated, _ = dvideo_env.step([action.item()])\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        step +=1\n",
    "        \n",
    "        if step > 200:\n",
    "            break\n",
    "\n",
    "    print(\"score: \", score, \"step:\", step)\n",
    "    dvideo_env.close()\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    ",«2q12yykm,,,,,,,,,,i89[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
