{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a65ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efadd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) 4 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(env.observation_space.shape, obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c559c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(ActionNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return F.softmax(self.layers(x))\n",
    "    \n",
    "class QValueNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        '''状态价值，用来评估动作的好坏程度'''\n",
    "        super(QValueNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca75d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActionNetwork(obs_dim, action_dim)\n",
    "critic = QValueNetwork(obs_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e97935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''策略网络做出决策，给出一个动作，并让智能体执行'''\n",
    "    action_probs = actor(torch.FloatTensor(state).to(device))\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    loss = m.log_prob(action)\n",
    "    return action.item(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "steps = []\n",
    "U_s = []\n",
    "view_losses = []\n",
    "max_epoch = 1000\n",
    "for i in range(max_epoch):\n",
    "    score = 0\n",
    "    step = 0\n",
    "    \n",
    "    state, _ = env.reset(seed=3)\n",
    "    trajectories = []\n",
    "    while True:\n",
    "        action, loss = select_action(state)\n",
    "        '''从环境中观测到奖励和新的状态'''\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        step += 1\n",
    "        score += reward\n",
    "        \n",
    "        trajectories.append([state, action, reward, loss])\n",
    "        if done or step > 200:\n",
    "            steps.append(step)\n",
    "            break\n",
    "        \n",
    "        '''根据策略网络做决策,但不让智能体执行动作'''\n",
    "        '''让价值网络打分'''\n",
    "        '''计算 TD 目标和 TD 误差'''\n",
    "        '''更新价值网络'''\n",
    "        '''更新策略网络'''\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
